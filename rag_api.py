from flask import Flask, request, jsonify
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.document_loaders import TextLoader
from flask_cors import CORS  # å¯¼å…¥ CORS

app = Flask(__name__)
CORS(app)
# 1. åŠ è½½æ•°æ®
file_path = "DATA.txt"
loader = TextLoader(file_path, encoding="utf-8")
documents = loader.load()

# 2. æ‹†åˆ†æ–‡æœ¬
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)

# 3. åµŒå…¥æ¨¡å‹
embedding_model = OllamaEmbeddings(model="nomic-embed-text")

# 4. å»ºç«‹å‘é‡æ•°æ®åº“
vector_db = FAISS.from_documents(chunks, embedding_model)

# 5. LLM æ¨¡å‹
model = OllamaLLM(model="llama3.2:3b")

# 6. æ„å»º RAG æ£€ç´¢é“¾
retriever = vector_db.as_retriever()
qa_chain = RetrievalQA.from_chain_type(model, retriever=retriever)

@app.route("/rag", methods=["POST"])
def rag_endpoint():
    data = request.json
    question = data.get("question", "")
    print(f"\nğŸ“¥ Received user input: {question}")
    if not question:
        return jsonify({"error": "Missing question"}), 400

    # æ‰§è¡Œ RAG
    result = qa_chain.run(question)
    print(f"\nğŸ“¥ Top relevant documents: {result}")
    return jsonify({
        "question": question,
        "context": result
    })

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
